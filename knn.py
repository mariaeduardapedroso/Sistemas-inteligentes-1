# -*- coding: utf-8 -*-
"""Knn.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EnOLtEKpy9pBNZKi0kMQ9XT2BVzhHXiD
"""

# Códificação

# Importa a biblioteca scikit-learn para datasets
from sklearn import datasets

# Importa a biblioteca NumPy com alias 'np'
import numpy as np

# Importa a biblioteca statistics para cálculos estatísticos
import statistics

# Importa a biblioteca seaborn para visualização de dados estatísticos
import seaborn as sns

# Importa a função confusion_matrix do módulo sklearn.metrics
from sklearn.metrics import confusion_matrix

# Importa a função fetch_california_housing do módulo sklearn.datasets
from sklearn.datasets import fetch_california_housing

# Importa a função train_test_split do módulo sklearn.model_selection
from sklearn.model_selection import train_test_split

# Importa a função mean_squared_error do módulo sklearn.metrics
from sklearn.metrics import mean_squared_error

# Importa o módulo pyplot da biblioteca Matplotlib com alias 'plt'
import matplotlib.pyplot as plt

# Especifica o número de vizinhos (k) como 3
k = 3

# Carrega o conjunto de dados Iris
iris = datasets.load_iris()

# Obtém as características e rótulos do conjunto de dados Iris
atributes = iris.data
labels = iris.target

# Cria um conjunto de dados no formato desejado
dataset = [{"Data": data, "Label": labels[index]} for index, data in enumerate(atributes)]

# Carrega o conjunto de dados California housing
housing = fetch_california_housing()

# Seleciona apenas uma característica para simplificar (renda média)
X = housing.data[:, [7]]

# Obtém os rótulos
y = housing.target

# Divide o conjunto de dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Separar o conjunto em treino e teste
train = dataset[0:30] + dataset[51:81] + dataset[101:131]
test  = dataset[30:51] + dataset[81:101] + dataset[131:]

# K-nn
def nashaKnn(exemplo, train, k=3):
    # Calcula a distância do exemplo para cada item no conjunto de treino
    # Não podemos incluir o rótulo no cálculo da distância
    distancias = []

    # Itera sobre os elementos no conjunto de treino
    for elemento in train:
        distancia = (
            (elemento["Data"][0] - exemplo["Data"][0]) ** 4 +
            (elemento["Data"][1] - exemplo["Data"][1]) ** 4 +
            (elemento["Data"][2] - exemplo["Data"][2]) ** 4 +
            (elemento["Data"][3] - exemplo["Data"][3]) ** 4
        ) ** (1/4)
        distancias.append(distancia)

    # Ordena as distâncias junto com os elementos do conjunto de treino
    knnNeighbors = sorted(zip(train, distancias), key=lambda x: x[1])[1:k+1]

    # Seleciona os k melhores rótulos
    predictionLabels = [element[0]["Label"] for element in knnNeighbors]

    # Retorna a previsão como o rótulo mais frequente (modo)
    prediction = statistics.mode(predictionLabels)

    return prediction

# DWNN
def nashaDwnn(exemplo, train, k=3):
    # Inicializa uma lista para armazenar as distâncias ponderadas
    distancias = []

    # Itera sobre os elementos no conjunto de treino
    for elemento in train:
        # Calcula a ponderação de cada distância
        classe = elemento["Label"]
        distanciaMinkowski = ((elemento["Data"][0] - exemplo["Data"][0]) ** 4 +
                     (elemento["Data"][1] - exemplo["Data"][1]) ** 4 +
                     (elemento["Data"][2] - exemplo["Data"][2]) ** 4 +
                     (elemento["Data"][3] - exemplo["Data"][3]) ** 4) ** (1/4)

        distanciaManhattan = (
                     abs(elemento["Data"][0] - exemplo["Data"][0]) +
                     abs(elemento["Data"][1] - exemplo["Data"][1]) +
                     abs(elemento["Data"][2] - exemplo["Data"][2]) +
                     abs(elemento["Data"][3] - exemplo["Data"][3])
                     )

        # Verifica se a distância é zero para evitar divisão por zero
        if distanciaMinkowski == 0:
            return classe

        # Calcula o peso da distância
        wi = 1 / distanciaMinkowski

        # Adiciona a distância ponderada e a classe à lista de distâncias
        distancias.append((distanciaManhattan * wi, classe))

    # Ordena as distâncias
    dwnnNeighbors = sorted(distancias, key=lambda x: x[0])[:k]

    # Seleciona as k melhores classes
    predictionLabels = [element[1] for element in dwnnNeighbors]

    # Retorna a previsão como a classe mais frequente (modo)
    prediction = statistics.mode(predictionLabels)

    return prediction

def plotConfusionMaxtrix3(predictionResults, labels):
    # Cria a matriz de confusão usando sklearn
    confusionMatrix = confusion_matrix([true_label for true_label, _ in predictionResults],
                                      [predicted_label for _, predicted_label in predictionResults])

    # Plota a matriz de confusão usando seaborn
    plt.figure(figsize=(6, 4))
    sns.heatmap(confusionMatrix, annot=True, fmt='d', cmap='Blues', cbar=False,
                xticklabels=[labels[0], labels[1], labels[2]],
                yticklabels=[labels[0], labels[1], labels[2]])
    plt.xlabel('Previsões')
    plt.ylabel('Rótulos Verdadeiros')
    plt.title('Matriz de Confusão')
    plt.show()

# Inicializa uma lista para armazenar os resultados das previsões
predictionResults = []

# Itera sobre os elementos no conjunto de teste
for testElement in test:
    # Obtém o rótulo verdadeiro do elemento de teste
    lab = testElement["Label"]

    # Realiza uma previsão usando o modelo nashaKnn e armazena o par (previsão, rótulo verdadeiro)
    predictionResults.append((nashaKnn(testElement, train, k), lab))

# Inicializa uma matriz de confusão com todas as entradas inicializadas como zero
confusionMatrix = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]

# Itera sobre os elementos em predictionResults
for element in predictionResults:
    # Incrementa a entrada correspondente na matriz de confusão
    confusionMatrix[element[0]][element[1]] += 1

# Imprime a matriz de confusão
print(confusionMatrix)

# Calcula a soma total da matriz de confusão
total_samples = sum(confusionMatrix[0]) + sum(confusionMatrix[1]) + sum(confusionMatrix[2])

# Imprime a soma total
print(total_samples)

plotConfusionMaxtrix3(predictionResults,iris.target_names)

# Inicializa uma lista para armazenar os resultados das previsões
predictionResults = []

# Itera sobre os elementos no conjunto de teste
for testElement in test:
    # Obtém o rótulo verdadeiro do elemento de teste
    lab = testElement["Label"]

    # Realiza uma previsão usando o modelo nashaDwnn e armazena o par (previsão, rótulo verdadeiro)
    predictionResults.append((nashaDwnn(testElement, train, k), lab))

# Inicializa uma matriz de confusão com todas as entradas inicializadas como zero
confusionMatrix = [[0, 0, 0], [0, 0, 0], [0, 0, 0]]

# Itera sobre os elementos em predictionResults
for element in predictionResults:
    # Incrementa a entrada correspondente na matriz de confusão
    confusionMatrix[element[0]][element[1]] += 1

# Imprime a matriz de confusão
print(confusionMatrix)

# Calcula a soma total da matriz de confusão
total_samples = sum(confusionMatrix[0]) + sum(confusionMatrix[1]) + sum(confusionMatrix[2])

# Imprime a soma total
print(total_samples)

plotConfusionMaxtrix3(predictionResults,iris.target_names)

# Implementação manual do KNN para regressão
def knn_regressao(X_treino, y_treino, X_teste, k):
    previsoes = []
    for x_q in X_teste:
        # Calcula as distâncias entre x_q e todos os pontos em X_treino
        distancias = np.linalg.norm(X_treino - x_q, axis=1)

        # Obtém os índices dos k vizinhos mais próximos
        indices = np.argsort(distancias)[:k]

        # Calcula a média dos rótulos dos k vizinhos mais próximos
        previsao = np.mean(y_treino[indices])
        previsoes.append(previsao)

    return np.array(previsoes)

# Faz previsões usando a implementação manual do KNN
y_predito = knn_regressao(X_train, y_train, X_test, k)

# Avalia o desempenho do modelo
mse = mean_squared_error(y_test, y_predito)
print(f'Erro Quadrático Médio: {mse}')

# Visualiza os resultados

# Plota as observações reais no gráfico de dispersão
plt.scatter(X_test, y_test, color='black', label='Observações reais')

# Plota as previsões do modelo KNN no gráfico de dispersão
plt.scatter(X_test, y_predito, color='blue', label='Previsões')

# Adiciona título ao gráfico
plt.title('KNN Regressão - Preços de Casas na Califórnia (Implementação Manual)')

# Adiciona rótulo ao eixo x
plt.xlabel('Renda Média')

# Adiciona rótulo ao eixo y
plt.ylabel('Preço da Casa')

# Adiciona legenda ao gráfico
plt.legend()

# Exibe o gráfico
plt.show()

# Carrega o conjunto de dados California housing
housing = fetch_california_housing()
X = housing.data[:, :]  # Usando todas as características disponíveis
y = housing.target

# Divide o conjunto de dados em conjuntos de treinamento e teste
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Implementação manual do Distance-Weighted Nearest Neighbors para regressão
def dw_knn_regressao(X_treino, y_treino, X_teste, k):
    previsoes = []
    for x_q in X_teste:
        # Calcula as distâncias entre x_q e todos os pontos em X_treino
        distancias = np.linalg.norm(X_treino - x_q, axis=1)

        # Obtém os índices dos k vizinhos mais próximos
        indices = np.argsort(distancias)[:k]

        # Calcula os pesos com base na inversa do quadrado da distância
        pesos = 1 / (distancias[indices] ** 2)

        # Calcula a média ponderada dos rótulos dos k vizinhos mais próximos
        previsao = np.sum(pesos * y_treino[indices]) / np.sum(pesos)
        previsoes.append(previsao)

    return np.array(previsoes)

# Faz previsões usando a implementação manual do Distance-Weighted KNN
y_predito_dw = dw_knn_regressao(X_train, y_train, X_test, k)

# Avalia o desempenho do modelo
mse_dw = mean_squared_error(y_test, y_predito_dw)
print(f'Erro Quadrático Médio (Distance-Weighted): {mse_dw}')

# Visualiza os resultados

# Plota as observações reais no gráfico de dispersão
plt.scatter(X_test[:, 0], y_test, color='black', label='Observações reais')

# Plota as previsões do modelo Distance-Weighted KNN no gráfico de dispersão
plt.scatter(X_test[:, 0], y_predito_dw, color='green', label='Previsões (Distance-Weighted)')

# Adiciona título ao gráfico
plt.title('Distance-Weighted KNN Regressão - Preços de Casas na Califórnia')

# Adiciona rótulo ao eixo x
plt.xlabel('Primeira Característica')

# Adiciona rótulo ao eixo y
plt.ylabel('Preço da Casa')

# Adiciona legenda ao gráfico
plt.legend()

# Exibe o gráfico
plt.show()